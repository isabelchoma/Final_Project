---
title: "README"
output:
 github_document:
 pandoc_args: ["--wrap=none"]
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

```{r, include=FALSE}
#Load the dataset
baseball_data <- read.csv('./Data/baseball_1985_2016.csv')

# Check if packages already installed, and install if not
if (!require("knitr")) install.packages("knitr"); library(knitr)
if (!require("ggplot2")) install.packages("ggplot2"); library(ggplot2)
if (!require("dplyr")) install.packages("dplyr"); library(dplyr)
if (!require("corrplot")) install.packages("corrplot"); library(corrplot)
if (!require("tidyr")) install.packages("tidyr"); library(tidyr)
if (!require("ggpubr")) install.packages("ggpubr"); library(ggpubr)
if (!require("stats")) install.packages("stats"); library(stats)  # See note below
if (!require("car")) install.packages("car"); library(car)
if (!require("kableExtra")) install.packages("kableExtra"); library(kableExtra)
if (!require("ggcorrplot")) install.packages("ggcorrplot"); library(ggcorrplot)
if (!require("tidyverse")) install.packages("tidyverse"); library(tidyverse)
if (!require("caret")) install.packages("caret"); library(caret)
if (!require("randomForest")) install.packages("randomForest"); library(randomForest)
if (!require("e1071")) install.packages("e1071"); library(e1071)

```

By Isabel Choma, Isabella Reid, Kiley Meringer, and Sophia Hess

## Introduction 

As college students, we are constantly busy with classes, homeworks, and exams. We are all very involved in extracurricular activities outside of classes, which takes up a lot of time. However, in the free time that we do have, we love to get together with our friends and watch sports games. Whether it is the fall semester or the spring semester, there is always a sport being played (football in the fall, basketball in the winter, baseball in the spring). Since this project is being completed in the spring season, we decided to focus our attention on baseball. 

Specifically, we are interested in learning more about what influences a baseball player’s salary. Is it their batting average? Their experience? Their age? The team they play for? We will analyze these variables, plus many others, and how they influence a player’s salary. In this case, we are interested in assessing the following question: What best influences a hitters’ baseball player’s salary? 

To start, we will understand the data by reading literature about impacts on a baseball player’s salary. This will allow us to absorb the material we are trying to learn more about before actually starting to import and analyze the data into an open source data software. Then, we will provide some descriptive and numeric statistics of the dataset we collected. This will help provide an overview of the important features of the variables. Next, we will do some exploratory data analysis, analyzing any correlations between the independent variables of interest and the dependent variable. We will utilize plots and graphs to help us visualize the data. Finally, we will use machine learning models to see what the best way to identify patterns, make predictions, and improve its performance over time. 

We will calculate the batting average by dividing each player’s total hits per season by their total at-bat. This batting average is different because a hit is a batter reaching first base. It is a good starting point for evaluating a player’s offensive skills. A higher batting average suggests a batter is more consistent in making solid contact with the ball and getting on base via hits. 

## Data Organization

Data was collected using a website called Baseball Reference, which has baseball history and statistics for Major League Baseball. It is the complete source for current and historical baseball players, teams, scores, and leaders. Specifically, our dataset focuses on statistics from 1985, showing individuals’ performance, age, and other relevant information for a singular season. 

According to the Bureau Labor of Statistics, salary trends change over time. With free agencies and salary arbitration, salaries escalate. In addition, TV revenues fuel salary expansion, as the League has more money to work with. In order to understand how much a team can spend on its players, it is important to understand the distribution of their money. For the owners, player compensation looks like this: Major League Player Compensation + Benefit Plan Costs + Postseason Share Payments + Minor League Signing Bonuses (not including associated tax) + Minor League Salaries And Benefits. 
	
So, in order to comprehend the salary of an individual player, we must take this distribution into account. Salaries to the players themselves are comparable to salaries in the entertainment industry: star performers disproportionately earn higher amounts. These higher amounts are influenced by performance, seniority, and market size, with performance being the dominant factor. 

```{r, include=FALSE}
#Keeping only selected variables to evaluate
hitters_data <- baseball_data %>%
  select(playerid, yearid, hits_p, game_p, stint, atbat_b, hr_b, rbi_b, salary, birthyear)  

#Log salary to scale the data
hitters_data$log_Salary <- log(hitters_data$salary)

#Replace original salary data with logged salary
hitters_data <- hitters_data %>%
  filter(salary > 0) %>%
  mutate(salary = log(salary))

#Delete duplicate row of logged salary
hitters_data <- hitters_data[, -9]
```


After filtering the data by removing the irrelevant attributes to our analysis, these are the attributes included in our data, as well as their data type:

```{r, include=FALSE}
colnames(hitters_data) <- c("Player_ID", "Year_ID", "Hits", "Games Played", "Stint", "At Bat", "Homeruns", "RBI", "Birth Year", "Logged_Salary")

#Feature info table 
feature_info <- data.frame(
  Feature = colnames(hitters_data),
  Description = c("Player ID", "Year ID", "Number of hits per season", "Games played per season","How many years player has been on team", "Number of times at bat per season", "Home runs per season","Runs Batted In per season", "Birth Year", "Yearly salary in dollars, logged" 
  ),
  Scale_Type = c("Character", "Ordinal", "Interval", "Interval", "Interval", "Interval", "Interval", "Interval", "Interval", "Ratio"
  )
)
```


### Feature Descriptions for Baseball Data

```{r, echo=FALSE}
# Display the table nicely
kable(feature_info)
```

Some of the players in the dataset were pitchers and not hitters, so we had to remove rows with NA values for hits in order to only look at the hitters. 

```{r, echo=FALSE}
# Remove any NA or NaN value
hitters_data <- hitters_data[!is.na(hitters_data$Hits), ]

# Function to summarize data with key statistics, excluding Player_ID from summary only
get_summary_stats <- function(data) {
  data %>%
    select(where(is.numeric), -Year_ID) %>%  # keep only numeric vars, EXCEPT Year_ID
    summarise(across(
      everything(),
      list(
        Count = ~sum(!is.na(.)),
        Mean = ~mean(., na.rm = TRUE),
        Median = ~median(., na.rm = TRUE),
        SD = ~sd(., na.rm = TRUE),
        Min = ~min(., na.rm = TRUE),
        Max = ~max(., na.rm = TRUE)
      ),
      .names = "{.col}_{.fn}"
    )) %>%
    pivot_longer(
      cols = everything(),
      names_to = c("Variable", "Statistic"),
      names_pattern = "^(.*)_(Count|Mean|Median|SD|Min|Max)$",
      values_to = "Value"
    ) %>%
    pivot_wider(names_from = Statistic, values_from = Value)
}

# Summary Table
summary_baseball <- get_summary_stats(hitters_data)

kable(summary_baseball, caption = "Detailed Summary Statistics") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = FALSE)

```

## Exploratory Data Analysis

### Distribution of Variables 

```{r, echo=FALSE}
# Histogram for Hits
ggplot(hitters_data, aes(x = Hits)) +
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Hits", x = "Hits", y = "Frequency") +
  theme_minimal()

# Density plot for Hits
ggplot(hitters_data, aes(x = Hits)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Density of Hits", x = "Hits", y = "Density") +
  theme_minimal()
```

The distribution of Hits appears right-skewed, with a larger number of players accumulating fewer hits. The peak of the histogram is centered around a moderate value, suggesting that most players have a relatively average number of hits. There is a tail extending towards higher values, which indicates that there are a small number of players with many hits. The spread of the data, particularly with the long tail on the higher end, suggests that there are outliers or a small group of players who have significantly more hits than the majority.

```{r, echo=FALSE}
# Histogram for Birth Year 
ggplot(hitters_data, aes(x = `Birth Year`)) +
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Birth Year", x = "Birth Year", y = "Frequency") +
  theme_minimal()

# Density plot for Birth Year 
ggplot(hitters_data, aes(x = `Birth Year`)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Density of Birth Year", x = "Birth Year", y = "Density") +
  theme_minimal()
```

The distribution of Birth Year shows the distribution of players' birth years, with clear peaks around the 1940s, 1960s, and 1980s. The highest frequencies occur for players born in the 1960s and 1980s, with a smaller peak for those born in the 1940s. Fewer players were born in the earlier (1930s) or later years (late 1990s and beyond), indicating that most players fall into these key birth-year ranges, likely reflecting the active playing years of those players in the dataset.

```{r, echo=FALSE}
# Histogram for At Bat
ggplot(hitters_data, aes(x = `At Bat`)) +
  geom_histogram(binwidth = 10, fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Distribution of At Bat", x = "At Bat", y = "Frequency") +
  theme_minimal()

# Density plot for At Bat
ggplot(hitters_data, aes(x = `At Bat`)) +
  geom_density(fill = "skyblue", color = "black", alpha = 0.7) +
  labs(title = "Density of At Bat", x = "At Bat", y = "Density") +
  theme_minimal()
```

The At Bat variable shows that most players have very few at-bats, with the distribution being heavily skewed towards lower values. Only a small subset of players have a significant number of at-bats, indicating a few high-usage players in contrast to many with minimal playing time.

```{r, echo=FALSE}
# Histogram for log_Salary
ggplot(hitters_data, aes(x = Logged_Salary)) +
  geom_histogram(binwidth = 0.1, fill = "lightgreen", color = "black", alpha = 0.7) +
  labs(title = "Distribution of Logged Salary", x = "Log Salary", y = "Frequency") +
  theme_minimal()

# Density plot for log_Salary
ggplot(hitters_data, aes(x = Logged_Salary)) +
  geom_density(fill = "lightgreen", color = "black", alpha = 0.7) +
  labs(title = "Density of Logged Salary", x = "Log Salary", y = "Density") +
  theme_minimal()

```

The distribution of logged salary shows a right-skewed distribution, with most players having salaries concentrated between log values of 12 and 13. This suggests that the majority of players earn salaries within a moderate range, while a small number of players have significantly higher salaries, creating a long right tail. This indicates the presence of outliers, typical of professional sports where a few players earn much more than the majority.

The variables are not normally distributed, so we cannot run tests that assume normality, such as the Shapiro-Wilk test or Q-Q plots.
Instead, we would use the Pearson Correlation test.

### Pearson Correlation heatmap

Before running any exploratory analysis, we decided to see if there are any strong correlations that stand out for our independent variables of interest and our dependent variable, logged salary. According to the heatmap scale, the more correlated the variables are, the darker the color will be. We also have the actual correlations between the variables presented in each box. 


```{r, echo=FALSE}
#Compute Pearson correlation
numeric_filtered_data <- hitters_data[sapply(hitters_data, is.numeric)]
cor_matrix <- cor(numeric_filtered_data, method = "pearson")

ggcorrplot(cor_matrix, lab = TRUE, type = "lower", title = "Pearson Correlation Heatmap")

```

Surprisingly, we found that there are not many strong correlations between the variables of interest and our dependent variable. Even though the strongest correlation between an independent variable and salary is Year ID at .46, this is not helpful to our interpretation of how strongly the data is correlated since we are including this variable to represent that this is not time series data. Therefore, this correlation is not important. The next strongest correlation is for the independent variable, hits, with a correlation coefficient	of .26. This means that out of all the independent variables we looked at, hits are most likely to predict the outcome of a players’ salary. This is as expected, as the more hits a player has, the more likely they are going to get paid higher salaries, since their overall performance is high. The correlation of 0.21 between Birth Year and logged salary is moderate and positive, meaning that players who are older (i.e., having a higher birth year number) tend to have higher logged salaries. This might reflect the effect of experience and seniority, as older players may earn higher salaries due to their experience in the league.The correlation between At Bat and logged salary is 0.11, which indicates a weak positive relationship. This implies that while there is a slight tendency for players with more at-bats to have a higher logged salary, the relationship is not substantial.

### Check statistical significance of variables

```{r, echo=FALSE}
model <- lm(Logged_Salary ~ Hits + `Games Played` + Stint + `At Bat` + Homeruns + RBI + `Birth Year`, data = hitters_data)

model_summary <- summary(model)

# Extract coefficients
coef_table <- as.data.frame(model_summary$coefficients)

# Optionally rename the columns for clarity
colnames(coef_table) <- c("Estimate", "Std. Error", "t value", "Pr(>|t|)")

# Load knitr for pretty printing
library(knitr)

# Print nicely formatted table
kable(coef_table, caption = "Linear Regression Results for Logged_Salary")

```

In order to test if the correlations between these variables and salary were significant, we ran hypothesis tests on each of the variables depicted in the heatmap. We ran a t-test to determine if the p-values were small enough for each variable to be statistically significant. After running these tests, Hits, Games Played, and Birth Year all have highly significant p-values (well below 0.001), meaning these variables strongly influence logged salary. Stint is also significant (p < 0.01), but its effect is negative. At Bat and Homeruns have non-significant p-values (above 0.05), meaning they do not meaningfully contribute to the model. RBI is marginally significant with a p-value close to 0.1, suggesting a weak but possibly important relationship with salary. 

Our analysis in the next section will include only the variables that had a statistically significant effect on salary. 

### Check Assumptions 

Because we ran statistical tests, we need to run tests to check for linearity, homoscedasicity, and multicollinearity. We found above that the variables are not normally distributed, so we are running non-parametric tests. 

```{r, echo=FALSE}
# Reduced model with only statistically significant variables
model_sig <- lm(Logged_Salary ~ Hits + `Games Played` + Stint + `Birth Year`, data = hitters_data)

# Check linearity 
plot(model_sig, which = 1)  # Residuals vs Fitted

# Check homoscedasticity
library(lmtest)
bptest(model_sig)  # Breusch-Pagan test

# Check multicollinearity
vif(model_sig)

```

Linearity:
The Residuals vs. Fitted plot shows the residuals (the differences between the observed and predicted values) on the y-axis and the fitted values (predicted values) on the x-axis. The plot displays a random scatter of points around the horizontal line at zero, indicating that the model is generally well-fitted. The slightly decreasing trend in the residuals, however, suggests a mild non-linear pattern that the model might not be capturing perfectly. This could be a signal to consider potential improvements, such as adding non-linear terms or interactions. Overall, the residuals seem evenly distributed around zero, implying no severe heteroskedasticity or major model misspecifications.

Breusch-Pagan Test:
Interpretation: The very small p-value (< 0.05) strongly suggests that the residuals do not have constant variance — i.e., heteroscedasticity is present.
Why it matters: This can lead to inefficient estimates and invalid p-values, so it's a critical issue.
Fix: Use robust standard errors when reporting inference

Variance Inflation Factor:
Interpretation: All VIFs are very close to 1, indicating no multicollinearity.
No action needed — this assumption is met.

### Scatterplots of statistically significant variables

```{r, echo=FALSE}
library(ggplot2)

# Scatterplot: Hits vs Logged_Salary
ggplot(hitters_data, aes(x = Hits, y = Logged_Salary)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(title = "Hits vs Logged Salary", x = "Hits", y = "Log(Salary)") +
  theme_minimal()
```

The scatterplot shows a positive relationship between Hits and Logged Salary, meaning that players with more hits tend to have higher logged salaries. While the correlation is evident, the scatter of the data points suggests that Hits is only one of several factors affecting salary.

```{r, echo=FALSE}
# Scatterplot: Hits vs Games Played
ggplot(hitters_data, aes(x = `Games Played`, y = Logged_Salary)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(title = "Games Played vs Logged Salary", x = "Games Played", y = "Log(Salary)") +
  theme_minimal()
```

The scatterplot shows a slight positive correlation between Games Played and Logged Salary, with higher numbers of games played tending to correspond to slightly higher salaries. However, the relationship is weak, and the data points are dispersed, indicating that Games Played is not a strong predictor of Logged Salary in this model.

```{r, echo=FALSE}
# Scatterplot: Hits vs Stint
ggplot(hitters_data, aes(x = Stint, y = Logged_Salary)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(title = "Stint vs Logged Salary", x = "Stint", y = "Log(Salary)") +
  theme_minimal()
```

The scatterplot indicates a negative relationship between the number of years played with the team (stint) and Logged Salary, meaning that as the number of years played with a specific team increases, the logged salary tends to decrease slightly. However, most data points are concentrated at Stint = 1, and only a small number of players have higher stint values. The relationship appears weak, and the trend is primarily driven by the concentration of players at the lower end of the Stint variable.

```{r, echo=FALSE}
# Scatterplot: Hits vs Birth Year
ggplot(hitters_data, aes(x = `Birth Year`, y = Logged_Salary)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method = "lm", se = TRUE, color = "blue") +
  labs(title = "Birth Year vs Logged Salary", x = "Birth Year", y = "Log(Salary)") +
  theme_minimal()

```

The scatterplot indicates a positive relationship between Birth Year and Logged Salary, where players born in later years tend to have higher salaries. The data points are closely aligned with the regression line, suggesting a stronger and more consistent relationship between these two variables compared to other variables like Hits or Games Played.

## Machine Learning

We are interested in predicting the salary outcome variable to forecast future outcomes based on learned patterns.

```{r, include=FALSE}

hitters_data2 <- hitters_data
colnames(hitters_data2)[4] = "Games_Played"
colnames(hitters_data2)[6] = "Birth_Year"
  
# Define formula
formula <- Logged_Salary ~ Hits + Games_Played + Stint + Birth_Year

# Split data: 60% train, 20% validation, 20% test
set.seed(123)
train_idx <- createDataPartition(hitters_data2$Logged_Salary, p = 0.6, list = FALSE)
train_data <- hitters_data2[train_idx, ]
temp_data <- hitters_data2[-train_idx, ]

# Now split remaining 40% equally into validation and test
validation_idx <- createDataPartition(temp_data$Logged_Salary, p = 0.5, list = FALSE)
validation_data <- temp_data[validation_idx, ]
test_data <- temp_data[-validation_idx, ]

```

We tried a linear model, which had low explanatory power, with an R-squared of 0.08878462, which means that the model explains only 8.87% of the variance in Logged_Salary.

RMSE = 1.08445407 (Root Mean Squared Error)
The model’s predictions are off by about 1.08 log-salary units on average.
Since Logged_Salary is a logarithmic scale, an error of 1.08 corresponds to a fairly large difference on the original salary scale (because exponentiating log differences increases spread).

MAE = 1.30518158 (Mean Absolute Error)
The average absolute prediction error is about 1.305 log-salary units, also large relative to log-salary range.

So, the model is under-fitting because the linear model can't capture possible non-linear relationships, interactions, and unmeasured factors, so we are using the randomForest test instead, which handles non-linearity and interactions. 

```{r, include=FALSE}

# Train Random Forest on TRAINING data
rf_model <- randomForest(formula, data = train_data, ntree = 500, importance = TRUE)

# Predict on training, validation, and test data
train_preds <- predict(rf_model, newdata = train_data)
validation_preds <- predict(rf_model, newdata = validation_data)
test_preds <- predict(rf_model, newdata = test_data)

# Create combined dataframe for plotting
plot_data <- data.frame(
  Actual = c(train_data$Logged_Salary, validation_data$Logged_Salary, test_data$Logged_Salary),
  Predicted = c(train_preds, validation_preds, test_preds),
  DataType = factor(rep(c("Train", "Validation", "Test"),
                        c(nrow(train_data), nrow(validation_data), nrow(test_data))))
)

# Calculate RMSE
rmse_train <- sqrt(mean((train_preds - train_data$Logged_Salary)^2))
rmse_validation <- sqrt(mean((validation_preds - validation_data$Logged_Salary)^2))
rmse_test <- sqrt(mean((test_preds - test_data$Logged_Salary)^2))

# Calculate R-squared
r2_train <- cor(train_preds, train_data$Logged_Salary)^2
r2_validation <- cor(validation_preds, validation_data$Logged_Salary)^2
r2_test <- cor(test_preds, test_data$Logged_Salary)^2

# Print results
cat("=== RANDOM FOREST PERFORMANCE ===\n")
cat("TRAIN SET:\n")
cat("   RMSE: ", round(rmse_train, 4), "\n")
cat("   R^2 : ", round(r2_train, 4), "\n\n")

cat("VALIDATION SET:\n")
cat("   RMSE: ", round(rmse_validation, 4), "\n")
cat("   R^2 : ", round(r2_validation, 4), "\n\n")

cat("TEST SET:\n")
cat("   RMSE: ", round(rmse_test, 4), "\n")
cat("   R^2 : ", round(r2_test, 4), "\n")


```

The Random Forest model shows moderate predictive performance across the train, validation, and test sets. The root mean square error (RMSE) remains consistent across all datasets—1.214 for training, 1.2665 for validation, and 1.255 for testing—indicating the model generalizes reasonably well without significant overfitting. However, the R² values are relatively low (0.2492 on training, 0.1418 on validation, and 0.1643 on testing), suggesting that the model explains only a small portion of the variance in the target variable. Overall, while the model is stable, its predictive power is limited, and further tuning or alternative modeling approaches may be needed to improve performance.

We also wanted to plot the model to see how it fits the data and how much it is under-fitting.

```{r, echo=FALSE}

# Plot: Predicted vs Actual for all three sets
ggplot(plot_data, aes(x = Actual, y = Predicted, color = DataType)) +
  geom_point(alpha = 0.6) +
  geom_abline(slope = 1, intercept = 0, color = "red", linetype = "dashed") +
  labs(title = "Random Forest: Predicted vs Actual (Train vs Validation vs Test)",
       x = "Actual Logged Salary",
       y = "Predicted Logged Salary") +
  theme_minimal() +
  theme(legend.title = element_blank()) +
  scale_color_manual(values = c("Train" = "blue", "Validation" = "orange", "Test" = "green"))


```

We can see from this plot that the predictions are clustered far from the diagonal, so the model is under-fitting 

## Conclusions 

The goal of our analysis was to analyze if there was a correlation between a variety of performance metrics and the player's logged salary. Our comprehensive analysis of MLB hitter salary determinants has yielded several important insights. Through both statistical analysis and machine learning approaches, we've identified key performance metrics that significantly influence player compensation.

The most influential factor in determining a hitter's salary is their number of hits in a season, confirming the fundamental importance of this basic performance metric in player valuation. This relationship makes intuitive sense since hits directly contribute to a team's offensive production and scoring potential. 

Player age (represented by birth year) emerged as the second most important predictor, with a complex relationship to salary. While experience typically commands higher compensation, our analysis revealed that prime-age players (in their late 20s to early 30s) generally earn more than both younger and older players, creating an inverted U-shaped relationship.

Games played also showed significant positive correlation with salary, indicating that availability and durability are highly valued by MLB teams. Players who consistently appear in the lineup command higher salaries, likely reflecting both their reliability and the cumulative impact of their contributions over a full season.

Interestingly, our analysis revealed that stint (years with current team) has a negative correlation with salary, suggesting that player movement between teams may be associated with salary increases. This could indicate that free agency and team changes often result in salary bumps as players leverage competitive offers.

These findings align with economic theories of labor markets in professional sports, where performance metrics serve as objective measures of productivity that determine compensation levels. Our machine learning models, particularly the Random Forest algorithm, demonstrated moderate predictive power for player salaries, explaining approximately 25% of the variance in compensation levels.

## Main Observations 

Performance-Based Compensation: Our analysis confirms that MLB player salaries are strongly linked to objective performance metrics, with batting performance (particularly hits) being the primary driver of compensation differences among players.

Age-Value Relationship: We observed a significant correlation between player age and salary, with peak earning potential typically occurring during a player's prime performance years rather than simply increasing with experience. This suggests teams invest most heavily in players they believe are at their performance peak.

Durability Premium: The strong influence of games played on salary highlights that availability is a valued attribute in MLB compensation structures. Players who remain healthy and consistently available command higher salaries, reflecting the value of reliability.

Non-Linear Relationships: Indicates that the relationships between performance metrics and compensation are complex and often non-linear, requiring sophisticated modeling approaches.

Team Loyalty Dynamics: The negative correlation between stint (years with the same team) and salary suggests interesting market dynamics where changing teams may benefit players financially. This could reflect the impact of free agency on player bargaining power.

Predictive Power: Our best model explained approximately 25% of salary variance, demonstrating that while performance metrics and player characteristics do influence compensation, other factors not captured in our dataset (such as market size, team budget constraints, or negotiation effectiveness) also play significant roles.

Salary Distribution: The right-skewed distribution of player salaries confirms the "superstar effect" in MLB, where top performers earn disproportionately more than average players, creating a highly unequal compensation structure typical of entertainment and professional sports.

## Future Directions

This information that we have extracted from the dataset can help determine how much players need to be contributing to their team in order to increase their salaries. It would now be interesting to look at the factors that affect a pitcher's salary, as well as the factors that affect a hitter's. This would differ since it would have to be specific to position in the outfield, for example a pitcher and a first baseman could not be judged on the same scale. There would need to be multiple forms for each position across teams and use variables like pitches, catches, forced outs, and others in order to look at the impact on their salary. 

## More Information 

Youtube video links:

Tutorial link: https://youtu.be/NEuI9wVOrjM 

Colab Link: https://colab.research.google.com/drive/1hX6l8H_OQYyWs576J1YHv_ALN1ycv54Z?authuser=2  

Link to io page: https://isabelchoma.github.io/Final_Project/ 

## Data Sources 

MLB Baseball Reference: https://www.baseball-reference.com/ 

## Acknowledgments 

Data Organization Article reference: 

BLS: https://www.bls.gov/opub/mlr/cwc/baseballs-changing-salary-structure.pdf 

Forbes: https://www.forbes.com/sites/maurybrown/2019/02/11/inside-the-numbers-the-player-salary-battle-lines-between-mlb-and-the-mlbpa/ 

